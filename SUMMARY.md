# Summary

* [Introduction](README.md)
* [Chapter I](chapter1.md)
* [Chapter II: Multi-arm Bandits](part1/chapter2/2_0.md)
  * [2.1 An n-Armed Bandit Problem](part1/chapter2/2_1.md)
  * [2.2 Action-Value Methods](part1/chapter2/2_2.md)
  * [2.3 Incremental Implementation](part1/chapter2/2_3.md)
  * [2.4 Tracking a Nonstationary Problem](part1/chapter2/2_4.md)
  * [2.5 Optimistic Initial Values](part1/chapter2/2_5.md)
  * [2.6 UCB Action Selection](part1/chapter2/2_6.md)
  * [2.7 Gradient Bandits](part1/chapter2/2_7.md)
  * [2.8 Associative Search](part1/chapter2/2_8.md)
* [Chapter III: Finite MDP](part1/chapter3/3_0.md)
  * [3.1 The Agent-Environment Interface](part1/chapter3/3_1.md)
  * [3.2 Goals and Rewards](part1/chapter3/3_2.md)
  * [3.3 Returns](part1/chapter3/3_3.md)
  * [3.4 Unified Notation](part1/chapter3/3_4.md)
  * [3.5 The Markov Property](part1/chapter3/3_5.md)
  * [3.6 Markov Decision Processes](part1/chapter3/3_6.md)
  * [3.7 Value Functions](part1/chapter3/3_7.md)
  * [3.8 Optimality Optimal Value Functions](part1/chapter3/3_8.md)
  * [3.9 Optimality and Approximation](part1/chapter3/3_9.md)
* [Chapter IV: Dynamic Programming](part1/chapter4/4_0.md)
  * [4.1 Policy Evaluation](part1/chapter4/4_1.md)
  * [4.2 Policy Improvement](part1/chapter4/4_2.md)
  * [4.3 Policy Iteration](part1/chapter4/4_3.md)
  * [4.4 Value Iteration](part1/chapter4/4_4.md)
  * [4.5 Asyncrhinous DP](part1/chapter4/4_5.md)
  * [4.6 Generalized Policy Iteration](part1/chapter4/4_6.md)
  * [4.7 Efficiency of DP](part1/chapter4/4_7.md)
* [Chapter V: Monte Carlo Methods](part1/chapter5/5_0.md)
  * [5.1 Monte Carlo Prediction](part1/chapter5/5_1.md)
  * [5.1 MC Estimation of Action values](part1/chapter5/5_2.md)
  * [5.3 MC Control](part1/chapter5/5_3.md)
